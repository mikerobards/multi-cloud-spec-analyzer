import os
import csv
import json
from typing import TypedDict
from dotenv import load_dotenv

# LangChain / LangGraph imports
from langgraph.graph import StateGraph, END
from langchain_google_vertexai import ChatVertexAI
from langchain_openai import AzureChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

# Load environment variables from a .env file if you have one
load_dotenv()

# --- CONFIGURATION ---
# 1. Google Cloud (The "Reader" Brain)
# We use Gemini 1.5 Pro because it has a massive context window (1M+ tokens),
# perfect for reading huge requirements documents.
# Ensure you run `gcloud auth application-default login` in your terminal first.
GCP_PROJECT_ID = os.getenv("GCP_PROJECT_ID", "your-google-cloud-project-id")
gemini_model = ChatVertexAI(
    model_name="gemini-2.5-flash",  # or "gemini-2.0-flash-exp"
    temperature=0,
    max_output_tokens=2048,
    project=GCP_PROJECT_ID,
    location="us-central1"
)

# 2. Azure OpenAI (The "Writer" Brain)
# We use GPT-4 on Azure to format the output. This simulates a common enterprise
# constraint where final artifacts must be generated by a specific compliant provider.
# If you don't have Azure keys yet, you can swap this class for standard `ChatOpenAI`.
azure_model = AzureChatOpenAI(
    azure_deployment=os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4"),
    openai_api_version=os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview"),
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT", "https://your-org.openai.azure.com/"),
    api_key=os.getenv("AZURE_OPENAI_API_KEY")
)

# --- THE AGENT STATE ---
# In LangGraph, "State" is the shared memory that passes between steps.
# It works like a dictionary that gets updated as the agent moves through the graph.
class AgentState(TypedDict):
    spec_text: str      # Input: The raw requirement text
    analysis_gaps: str  # Intermediate: The gaps found by Gemini
    ado_tickets: str    # Output: The final tickets formatted by Azure

# --- NODES (The Steps) ---

def analyze_requirements_node(state: AgentState):
    """
    Step 1: Use Google Gemini to analyze the spec for missing details.
    """
    print("--- STEP 1: Google Gemini is analyzing the requirements... ---")
    
    spec = state['spec_text']
    
    prompt = f"""
    You are a Senior Solutions Architect. Analyze the following project requirement text.
    Identify technical gaps, missing acceptance criteria, and vague statements.
    
    REQUIREMENT TEXT:
    {spec}
    """
    
    # Invoke Gemini
    response = gemini_model.invoke([HumanMessage(content=prompt)])
    
    # Update the state with the analysis
    return {"analysis_gaps": response.content}

def draft_tickets_node(state: AgentState):
    """
    Step 2: Use Azure OpenAI to take the analysis and write Azure DevOps tickets.
    """
    print("--- STEP 2: Azure OpenAI is drafting Azure DevOps tickets... ---")

    analysis = state['analysis_gaps']
    original_spec = state['spec_text']

    prompt = f"""
    You are a Technical Product Owner.
    Based on the original request and the Architect's gap analysis below,
    write 3-5 structured Azure DevOps Work Items.

    Return ONLY a valid JSON array. Each work item must have these exact fields:
    - "Work Item Type": Must be one of: "User Story", "Task", "Bug", "Feature"
    - "Title": A clear, concise title (max 100 characters)
    - "Description": Detailed description of the work item
    - "Acceptance Criteria": Bulleted list of criteria (use "- " for bullets)
    - "Priority": Must be one of: "1", "2", "3", "4"

    ORIGINAL REQUEST:
    {original_spec}

    ARCHITECT'S ANALYSIS:
    {analysis}

    Return ONLY the JSON array, no additional text or markdown code blocks.
    """

    # Invoke Azure OpenAI
    response = azure_model.invoke([HumanMessage(content=prompt)])

    # Update the state with the final tickets
    return {"ado_tickets": response.content}

def export_to_ado_csv(tickets_json: str, output_file: str = "ado_work_items.csv"):
    """
    Exports the JSON tickets to a CSV file compatible with Azure DevOps import.

    Args:
        tickets_json: JSON string containing the work items
        output_file: Path to the output CSV file

    Returns:
        Path to the created CSV file
    """
    try:
        # Clean up the JSON string (remove markdown code blocks if present)
        cleaned_json = tickets_json.strip()
        if cleaned_json.startswith("```json"):
            cleaned_json = cleaned_json.split("```json")[1]
        if cleaned_json.startswith("```"):
            cleaned_json = cleaned_json.split("```")[1]
        if cleaned_json.endswith("```"):
            cleaned_json = cleaned_json.rsplit("```", 1)[0]
        cleaned_json = cleaned_json.strip()

        # Parse the JSON
        tickets = json.loads(cleaned_json)

        # Azure DevOps CSV format headers
        fieldnames = [
            "Work Item Type",
            "Title",
            "Description",
            "Acceptance Criteria",
            "Priority"
        ]

        # Write to CSV
        with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()

            for ticket in tickets:
                writer.writerow({
                    "Work Item Type": ticket.get("Work Item Type", "User Story"),
                    "Title": ticket.get("Title", ""),
                    "Description": ticket.get("Description", ""),
                    "Acceptance Criteria": ticket.get("Acceptance Criteria", ""),
                    "Priority": ticket.get("Priority", "2")
                })

        print(f"\nâœ“ Successfully exported {len(tickets)} work items to: {output_file}")
        return output_file

    except json.JSONDecodeError as e:
        print(f"\nâœ— Error parsing JSON: {e}")
        print(f"Response content:\n{tickets_json}")
        # Fallback: Save the raw output to a text file
        fallback_file = "ado_work_items_raw.txt"
        with open(fallback_file, 'w', encoding='utf-8') as f:
            f.write(tickets_json)
        print(f"Saved raw output to: {fallback_file}")
        return None
    except Exception as e:
        print(f"\nâœ— Error exporting to CSV: {e}")
        return None

# --- THE GRAPH (The Architecture) ---
# This defines the workflow: Start -> Analyze -> Draft -> End

workflow = StateGraph(AgentState)

# Add our nodes
workflow.add_node("analyze_spec", analyze_requirements_node)
workflow.add_node("create_tickets", draft_tickets_node)

# Add edges (Connect the dots)
workflow.set_entry_point("analyze_spec") # Start here
workflow.add_edge("analyze_spec", "create_tickets") # Then go here
workflow.add_edge("create_tickets", END) # Then finish

# Compile the graph into a runnable application
app = workflow.compile()

# --- MAIN EXECUTION ---
if __name__ == "__main__":
    # Simulate a vague requirement input (typical PO scenario)
    sample_spec = """
    We need to migrate the 'Customer Loyalty' SQL database from on-prem
    to the cloud. It needs to work with the new mobile app.
    Make sure it's secure.
    """

    print(f"INPUT SPEC: {sample_spec.strip()}\n")

    # Run the graph
    result = app.invoke({"spec_text": sample_spec})

    print("\n\n################ FINAL OUTPUT ################\n")
    print(result['ado_tickets'])
    print("\n##############################################")

    # Export to CSV for Azure DevOps import
    csv_file = export_to_ado_csv(result['ado_tickets'])

    if csv_file:
        print(f"\nðŸ“‹ Import Instructions:")
        print(f"   1. Go to Azure DevOps > Boards > Work Items")
        print(f"   2. Click 'Import Work Items' or use the Excel plugin")
        print(f"   3. Upload the file: {csv_file}")
        print(f"   4. Map the columns if prompted")
        print(f"   5. Complete the import")